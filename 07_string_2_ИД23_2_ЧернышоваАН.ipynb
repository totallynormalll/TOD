{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppularxqn8xy"
      },
      "source": [
        "# Введение в обработку текста на естественном языке"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BJMYyZYn8yB"
      },
      "source": [
        "Материалы:\n",
        "* Макрушин С.В. Лекция 9: Введение в обработку текста на естественном языке\\\n",
        "* https://realpython.com/nltk-nlp-python/\n",
        "* https://scikit-learn.org/stable/modules/feature_extraction.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6PXjKsin8yE"
      },
      "source": [
        "## Задачи для совместного разбора"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vCGbtFJSn8yH"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pymorphy2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFeIGWeCn8yM"
      },
      "source": [
        "1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWasuvsVn8yN"
      },
      "outputs": [],
      "source": [
        "text = '''с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esI1ta5Jn8yP"
      },
      "source": [
        "2. Разбейте текст из формулировки задания 1 на слова; проведите стемминг и лемматизацию слов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rKhly_dn8yR"
      },
      "source": [
        "3. Преобразуйте предложения из формулировки задания 1 в векторы при помощи `CountVectorizer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JGTsWMAn8yT"
      },
      "source": [
        "## Лабораторная работа 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKi4-3WTn8yV"
      },
      "source": [
        "### Расстояние редактирования"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO6Dkqran8yW"
      },
      "source": [
        "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaStj3Yfn8yX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmN-cBWsn8yY",
        "outputId": "0109f981-ad65-4c06-ea23-f1b7e740f4b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "24485"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "data = pd.read_csv('preprocessed_descriptions.csv')\n",
        "description = data.preprocessed_descriptions.astype(str)\n",
        "\n",
        "words = []\n",
        "for i in description:\n",
        "    # Приводим к нижнему регистру и удаляем знаки пунктуации\n",
        "    i = i.lower()\n",
        "    i = ''.join([char for char in i if char not in string.punctuation])\n",
        "\n",
        "    words.extend(word_tokenize(i))\n",
        "\n",
        "unique_words = list(set(words))\n",
        "len(unique_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW3WHjScn8yg"
      },
      "source": [
        "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_m_yrdAn8yh"
      },
      "outputs": [],
      "source": [
        "from nltk.metrics.distance import (\n",
        "    edit_distance,\n",
        "    edit_distance_align,\n",
        "    binary_distance,\n",
        "    jaccard_distance,\n",
        "    masi_distance,\n",
        "    interval_distance,\n",
        "    custom_distance,\n",
        "    presence,\n",
        "    fractional_presence,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT58urU7n8yi",
        "outputId": "767e4841-b91e-4239-b4da-3d3dff36f66b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Расстояние между словами relyea и tunnel: 5\n",
            "Расстояние между словами pail и shaily: 3\n",
            "Расстояние между словами quck и technique: 8\n",
            "Расстояние между словами wife и kindof: 5\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "random_words = random.sample(unique_words, 10)\n",
        "for i in range(0, len(random_words)-3, 2):\n",
        "    distance = edit_distance(random_words[i], random_words[i+1])\n",
        "    print(f'Расстояние между словами {random_words[i]} и {random_words[i+1]}: {distance}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ot_2-4mn8yj"
      },
      "source": [
        "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dSe3rjmn8yk"
      },
      "outputs": [],
      "source": [
        "def nearest_words(word, k):\n",
        "    dictance_dict = {w:edit_distance(word, w) for w in words}\n",
        "    sorted_dict = dict(sorted(dictance_dict.items(), key=lambda x: x[1]))\n",
        "    for key in list(sorted_dict.keys())[:k]:\n",
        "        print(f'Расстояние от {word} до {key} равно {sorted_dict[key]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8h2VlnIn8yl",
        "outputId": "57a0944a-d67d-4271-9001-6abd95dd61cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Расстояние от freedom до fereedom равно 1\n",
            "Расстояние от freedom до fredom равно 1\n",
            "Расстояние от freedom до fredeom равно 2\n"
          ]
        }
      ],
      "source": [
        "words = ['rfeeodmm', 'fereedom', 'fredeom', 'fredom',\n",
        "         'fredoom', 'frrteddmon']\n",
        "\n",
        "nearest_words('freedom', 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zKqgB1zn8yn"
      },
      "source": [
        "### Стемминг, лемматизация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmZyKxV7n8yo"
      },
      "source": [
        "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами:\n",
        "    * word\n",
        "    * stemmed_word\n",
        "    * normalized_word\n",
        "\n",
        "Столбец `word` укажите в качестве индекса.\n",
        "\n",
        "Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u0Xf3Ndn8yp"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Инициализация SnowballStemmer и WordNetLemmatizer\n",
        "snb_stemmer = SnowballStemmer('english')\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJJjmP2Fn8yq",
        "outputId": "7b7443a5-87f4-4e47-87dc-ea4ba623f992"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stemmed_word</th>\n",
              "      <th>normalized_word</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>favourtie</th>\n",
              "      <td>favourti</td>\n",
              "      <td>favourtie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>taylor</th>\n",
              "      <td>taylor</td>\n",
              "      <td>taylor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>enchilladas</th>\n",
              "      <td>enchillada</td>\n",
              "      <td>enchilladas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flower7</th>\n",
              "      <td>flower7</td>\n",
              "      <td>flower7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>originality</th>\n",
              "      <td>origin</td>\n",
              "      <td>originality</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maggie</th>\n",
              "      <td>maggi</td>\n",
              "      <td>maggie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>minis</th>\n",
              "      <td>mini</td>\n",
              "      <td>mini</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>slurry</th>\n",
              "      <td>slurri</td>\n",
              "      <td>slurry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>protein</th>\n",
              "      <td>protein</td>\n",
              "      <td>protein</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>breathes</th>\n",
              "      <td>breath</td>\n",
              "      <td>breathes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24485 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            stemmed_word normalized_word\n",
              "word                                    \n",
              "favourtie       favourti       favourtie\n",
              "taylor            taylor          taylor\n",
              "enchilladas   enchillada     enchilladas\n",
              "flower7          flower7         flower7\n",
              "originality       origin     originality\n",
              "...                  ...             ...\n",
              "maggie             maggi          maggie\n",
              "minis               mini            mini\n",
              "slurry            slurri          slurry\n",
              "protein          protein         protein\n",
              "breathes          breath        breathes\n",
              "\n",
              "[24485 rows x 2 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stemmed_word = [snb_stemmer.stem(word) for word in unique_words]\n",
        "normalized_word = [lemmatizer.lemmatize(word) for word in unique_words]\n",
        "\n",
        "# Создание DataFrame с одинаковой длиной данных и индекса\n",
        "data = {'stemmed_word': stemmed_word, 'normalized_word': normalized_word}\n",
        "words_df = pd.DataFrame(data, index=unique_words)\n",
        "words_df.index.name = 'word'\n",
        "\n",
        "words_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKN25IQEn8yr"
      },
      "source": [
        "2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6bTVvlTn8ys",
        "outputId": "c4ae9a81-a57b-4489-ad70-eb4d4deaef9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Доля стоп-слов в общем количестве слов: 0.4711034401529359\n",
            "Топ-10 слов до удаления стоп-слов: [('the', 40413), ('a', 35131), ('and', 30585), ('i', 27945), ('this', 27181), ('to', 23598), ('it', 23300), ('is', 20306), ('of', 18405), ('for', 16023)]\n",
            "Топ-10 слов после удаления стоп-слов: [('recipe', 15198), ('make', 6438), ('time', 5287), ('use', 4652), ('great', 4522), ('like', 4276), ('easy', 4263), ('one', 4018), ('good', 3887), ('made', 3874)]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "#nltk.download('stopwords')\n",
        "\n",
        "# Загрузка данных\n",
        "data = pd.read_csv('preprocessed_descriptions.csv')\n",
        "description = data.preprocessed_descriptions.fillna('').astype(str)\n",
        "\n",
        "# Удаление стоп-слов\n",
        "stop_words = set(stopwords.words('english'))\n",
        "description_without_stopwords = description.apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
        "\n",
        "# Подсчет доли стоп-слов в общем количестве слов\n",
        "total_words = ' '.join(description).split()\n",
        "stop_words_count = sum(1 for word in total_words if word.lower() in stop_words)\n",
        "stop_words_fraction = stop_words_count / len(total_words)\n",
        "\n",
        "print(\"Доля стоп-слов в общем количестве слов:\", stop_words_fraction)\n",
        "\n",
        "# Топ-10 слов до удаления стоп-слов\n",
        "all_words = ' '.join(description).split()\n",
        "word_freq_before_removal = nltk.FreqDist(all_words)\n",
        "print(\"Топ-10 слов до удаления стоп-слов:\", word_freq_before_removal.most_common(10))\n",
        "\n",
        "# Топ-10 слов после удаления стоп-слов\n",
        "all_words_without_stopwords = ' '.join(description_without_stopwords).split()\n",
        "word_freq_after_removal = nltk.FreqDist(all_words_without_stopwords)\n",
        "print(\"Топ-10 слов после удаления стоп-слов:\", word_freq_after_removal.most_common(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhv2KxHan8yt"
      },
      "source": [
        "### Векторное представление текста"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N7_xEO9n8yu"
      },
      "source": [
        "3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rN_50Lan8yu",
        "outputId": "ca3354a8-bb2a-4dcf-823c-7594030eb73e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        add      also  although   another       any     bacon   barilla  \\\n",
            "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.387962   \n",
            "4  0.107773  0.107773  0.107773  0.107773  0.107773  0.107773  0.000000   \n",
            "\n",
            "         be       big     bread  ...      very      want   website      well  \\\n",
            "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "1  0.212982  0.212982  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "2  0.000000  0.000000  0.238709  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "4  0.000000  0.000000  0.000000  ...  0.107773  0.107773  0.107773  0.107773   \n",
            "\n",
            "      which      will      with      work       you       zwt  \n",
            "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "1  0.000000  0.212982  0.000000  0.000000  0.171833  0.000000  \n",
            "2  0.000000  0.000000  0.192589  0.000000  0.000000  0.238709  \n",
            "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "4  0.107773  0.000000  0.086951  0.107773  0.260852  0.000000  \n",
            "\n",
            "[5 rows x 100 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "random_recipe = data.sample(5)\n",
        "\n",
        "# Создание объекта TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Преобразование описаний в числовые вектора\n",
        "tfidf_matrix = vectorizer.fit_transform(random_recipe['preprocessed_descriptions'])\n",
        "\n",
        "# Получение имен признаков (слов) из TfidfVectorizer\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Создание DataFrame из числовых векторов\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
        "print(tfidf_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OMF9JvOn8yv"
      },
      "source": [
        "3.2 Вычислите близость между каждой парой рецептов, выбранных в задании 3.1, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4GyYWLon8yw"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "data = pd.read_csv('preprocessed_descriptions.csv')\n",
        "\n",
        "# Заполнение пропущенных значений пустой строкой\n",
        "data['preprocessed_descriptions'] = data['preprocessed_descriptions'].fillna('')\n",
        "\n",
        "\n",
        "# Создание объекта TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Преобразование описаний в числовые вектора\n",
        "tfidf_matrix = vectorizer.fit_transform(data['preprocessed_descriptions'])\n",
        "\n",
        "# Создание всех возможных пар рецептов\n",
        "pairs = list(itertools.combinations(data['name'], 2))\n",
        "\n",
        "# Вычисление близости между парами рецептов\n",
        "similarities = []\n",
        "for pair in pairs:\n",
        "    index1 = data.index[data['name'] == pair[0]].tolist()[0]\n",
        "    index2 = data.index[data['name'] == pair[1]].tolist()[0]\n",
        "    similarity = 1 - cosine(tfidf_matrix[index1].toarray().ravel(), tfidf_matrix[index2].toarray().ravel())\n",
        "    similarities.append([pair[0], pair[1], similarity])\n",
        "\n",
        "# Создание DataFrame для близости между рецептами\n",
        "similarities_df = pd.DataFrame(similarities, columns=['Recipe 1', 'Recipe 2', 'Similarity'])\n",
        "print(similarities_df)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}